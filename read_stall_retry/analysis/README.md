GCSFuse Stalled Read Retry Analysis Toolkit
============================================

This directory contains a set of scripts designed to fetch and analyze Google Cloud Storage FUSE (GCSFuse) logs from Google Cloud Logging. The tools help in identifying and visualizing the frequency and distribution of retries caused by stalled read requests.

The workflow is designed to be simple: fetch_logs.sh downloads the necessary logs to a pre-defined location (/tmp/), and the Python analysis scripts automatically find and process those logs.


Scripts Overview
----------------

1. fetch_logs.sh: A bash script that queries Google Cloud Logging to download specific GCSFuse "stalled read-req" log entries. It saves these logs into a CSV file at /tmp/<job_name>-logs.csv and prints the total count of retries found for stalled read requests.
2. retries_per_interval.py:  A Python script that processes the /tmp/<job_name>-logs.csv file to aggregate the "stalled read-req" retry counts into user-defined time intervals. It then generates a CSV file with the aggregated data and a bar chart visualization showing how the frequency of retries changes across these intervals.
3. requests_per_retry_count.py: A Python script that processes the /tmp/<job_name>-logs.csv file (generated by fetch_logs.sh) to analyze the "stalled read-req" retries. It identifies unique requests by their UUID and counts how many times each request was retried. The script then provides a summary table showing the distribution of retries per request (e.g., how many requests experienced a specific number of retries).


Prerequisites
-------------

Before you begin, ensure you have the following installed and configured:

* Google Cloud SDK (gcloud): Required by fetch_logs.sh to query logs from Google Cloud Logging. You must be authenticated (gcloud auth login) and have the necessary permissions.
  - Installation Guide (https://cloud.google.com/sdk/docs/install)

* Python 3: Required to run the analysis scripts.

* Python Libraries: The analysis scripts depend on pandas and matplotlib. You can install them using the provided requirements.txt file:
    
    pip install -r requirements.txt


Usage Workflow
--------------

The intended workflow is a 3-step process:

**Step 1: Fetch Logs**

First, use the fetch_logs.sh script to download the GCSFuse logs. The script requires you to provide a job_name, which will be used to create the log file in the /tmp directory.\
**Note:** Before running, make sure the script is executable:\
`chmod +x fetch_logs.sh`

Syntax:

    ./fetch_logs.sh <cluster_name> <job_name> <start_time> <end_time>

Example:

    ./fetch_logs.sh xpk-large-scale-usc1f-a sample-job "2025-02-04T18:00:00+05:30" "2025-02-05T10:00:00+05:30"

Output:

* A file named /tmp/sample-job-logs.csv will be created. The Python scripts below depend on this exact path and naming convention.

**Step 2: Analyze and Visualize Retries Over Time**

`retries_per_interval.py` is used to see how the frequency of retries changes over different time intervals. Provide the same job_name you used in Step 1.\
**Note:** You must run `fetch_logs.sh` successfully before using this script, as it depends on the log file created in the first step.

Syntax:

    ./retries_per_interval.py <job_name> [--interval <interval>]

Example:

    # This script will automatically read from /tmp/sample-job-logs.csv
    ./retries_per_interval.py sample-job --interval 5m

Output:

1. <job_name>-retries.csv: A CSV file in your current directory with the following format:
```
Interval Start (UTC),Retries
2025-05-14 16:31:00,4
2025-05-14 16:32:00,14
2025-05-14 16:33:00,12
2025-05-14 16:34:00,2299
2025-05-14 16:35:00,606
```
2. <job_name>-retries.png: A bar chart visualizing the retries over different time intervals, saved in your current directory.

**Step 3: Analyze Unique Requests per Retry Count**

`requests_per_retry_count.py` analyzes the logs to determine how many unique requests were retried a specific number of times. It generates a summary table showing, for example, how many requests were retried exactly once, exactly twice, and so on. Provide the same job_name you used in Step 1.\
**Note:** You must run `fetch_logs.sh` successfully before using this script.

Syntax:

    ./requests_per_retry_count.py <job_name>

Example:

    # This script will also read from /tmp/sample-job-logs.csv
    ./requests_per_retry_count.py sample-job

Output:

* A summary table printed to the console:

    Processing file: /tmp/sample-job-logs.csv
```
    Retries    | Requests
    -----------+----------
    1          | 248
    2          | 32
    3          | 10
```
