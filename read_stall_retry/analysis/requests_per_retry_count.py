#!/usr/bin/env python3

# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

r"""
This script processes a CSV log file generated by `fetch_logs.sh` and analyzes
"stalled read-req" events in GCSFuse. It identifies unique read requests by
UUID and counts how many times each request was retried.

It then summarizes the number of requests that were retried 1, 2, 3, ... times.

The input log file path is provided as a command-line argument.

Usage:
    python requests_per_retry_count.py <path_to_log_file>

Example:
    python requests_per_retry_count.py /tmp/sample-logs.csv

Output:
    Processing file: /tmp/sample-logs.csv

    Retries    | Requests
    -----------+----------
    1          | 248
    2          | 32
    3          | 10

    Each row in the table shows how many requests had a specific number of retries.
"""

import pandas as pd
import re
import sys
import argparse
from collections import defaultdict

LOG_PATTERN = re.compile(r'\[(.*?)\] stalled read-req cancelled after')

def main():
    parser = argparse.ArgumentParser(
        description=(
            "Analyze 'stalled read-req' log messages in GCSFuse logs.\n"
            "Counts retries per unique request (UUID) and summarizes the request distribution over retry frequency."
        ),
        epilog=(
            "Example usage:\n"
            "  python requests_per_retry_count.py /tmp/sample-logs.csv\n\n"
            "This will read the specified CSV file and output a summary of retry counts."
        ),
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        "log_file_path",
        help="Path to the CSV log file to be processed."
    )

    args = parser.parse_args()

    log_file = args.log_file_path

    print(f"Processing file: {log_file}")

    try:
        df = pd.read_csv(log_file)
    except FileNotFoundError:
        print(f"Error: Log file '{log_file}' not found.")
        sys.exit(1)
    except Exception as e:
        print(f"Error reading log file '{log_file}': {e}")
        sys.exit(1)

    # Sanitize header for case-insensitive and whitespace-proof comparison.
    sanitized_header = [h.strip().lower() for h in df.columns]

    # Validate that the header has the expected columns.
    if len(sanitized_header) < 2 or sanitized_header[0] != 'timestamp' or sanitized_header[1] != 'textpayload':
        print(f"Error: Invalid CSV header in '{log_file}'.", file=sys.stderr)
        print("Expected header to start with 'timestamp,textPayload'.", file=sys.stderr)
        print(f"Actual header: {','.join(df.columns)}", file=sys.stderr)
        sys.exit(1)

    retry_counts = defaultdict(int)

    for text in df['textPayload']:
        # Ensure text is a string before searching
        if not isinstance(text, str):
            continue
        match = LOG_PATTERN.search(text)
        if match:
            uuid = match.group(1)
            retry_counts[uuid] += 1

    if not retry_counts:
        print("No retries found in the log file.")
        sys.exit(0)

    frequency_counts = defaultdict(int)
    for count in retry_counts.values():
        frequency_counts[count] += 1

    # Print formatted table
    print(f"\n{'Retries':<10} | {'Requests':<10}")
    print(f"{'-'*10}-+-{'-'*10}")

    for retries_count in sorted(frequency_counts.keys()):
        requests_count = frequency_counts[retries_count]
        print(f"{retries_count:<10} | {requests_count:<10}")

if __name__ == "__main__":
    main()
